{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Nano-GPT producing Shakespeare text"
      ],
      "metadata": {
        "id": "-r-98x8tlhVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all the necessary libraries\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "UoqnDB88aI3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64                       # number of independent sequences in text\n",
        "block_size = 256                      # maximum context length of prediction\n",
        "max_iters = 3000                      # number of iteration\n",
        "eval_interval = 500                   # interval for evaluation\n",
        "learning_rate = 3e-4                  # learning rate for optimizer\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200                      # number of iteration for evaluation\n",
        "n_embd = 384                          # number of embedding dimensions --> for one word\n",
        "n_head = 6                            # number of heads in one attention block --> small parallel layers in one block\n",
        "n_layer = 6                           # number of different self-attention blocks all together\n",
        "dropout = 0.2                         # dropout probability"
      ],
      "metadata": {
        "id": "Ca2EKixjaMwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)   # random number seed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXg2Rvczdpcf",
        "outputId": "2da06641-12f8-4b0d-956f-788022a72624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x799ecc39de50>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "bFgtclBZdsEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)  # size of the vocabulary"
      ],
      "metadata": {
        "id": "Q5R7ilbFdxXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]           # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "TSqGK2D6d0PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]  # 90% for training\n",
        "val_data = data[n:]    # 10% for validation"
      ],
      "metadata": {
        "id": "DZCc54e9eBd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "sEq2huDDeC9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gives a better estimate of the loss function by nullifying the noise\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()                         # sets model to evaluation phase\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()                       # sets model to training phase\n",
        "    return out"
      ],
      "metadata": {
        "id": "fPG6yq3ieUlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one Head of each Self-Attention block\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):                              # head_size = n_embed // n_head\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)     # n_embed --> head_size\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)   # n_embed --> head_size\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)   # n_embed --> head_size\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch_size, block_size(T), channels(vocab_size=65))\n",
        "        # output of size (batch_size, block_size, head_size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)                                         # (batch_size, block_size, head_size)\n",
        "        q = self.query(x)                                       # (batch_size, block_size, head_size)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5        # (batch_size, block_size, head_size) @ (batch_size, head_size, block_size) --> (batch_size, block_size, block_size)\n",
        "        # so here for a specific batch, we multiply (block_size, head_size) @ (head_size, block_size) --> (block_size, block_size)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (batch_size, block_size, block_size)\n",
        "        wei = F.softmax(wei, dim=-1)                            # (batch_size, block_size, block_size)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)                                       # (batch_size, block_size, head_size)\n",
        "        out = wei @ v                                           # (batch_size, block_size, block_size) @ (batch_size, block_size, head_size) --> (batch_size, block_size, head_size)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ivfQin3WekdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  multiple self-attention heads in parallel\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # different heads\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # concatenate h for all heads vertically\n",
        "        # --> dimension = head_size*num_heads (because of each head * because of many heads)\n",
        "        out = self.dropout(self.proj(out))                   # (batch_size, block_size, n_embd)  --> because of projection\n",
        "        return out"
      ],
      "metadata": {
        "id": "h2Dm2gVEf2Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple feedfprward layer\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),                   # n_embed --> 4*n_embed  (according to transformers paper)\n",
        "            nn.ReLU(),                                       # ReLU activation function for non-linearity\n",
        "            nn.Linear(4 * n_embd, n_embd),                   # 4*n_embed --> n_embed  (according to transformers paper)\n",
        "            nn.Dropout(dropout),                             # dropout some connections with prob = dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)                                   # create the neural net"
      ],
      "metadata": {
        "id": "Zj7HjEY9gpIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one single block containing self-attention, feed forward, layer-norm etc etc\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head                         # n_embd = head_size*n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)      # self-attention\n",
        "        self.ffwd = FeedFoward(n_embd)                       # feed-forward network\n",
        "        self.ln1 = nn.LayerNorm(n_embd)                      # layer-norm\n",
        "        self.ln2 = nn.LayerNorm(n_embd)                      # layer-norm\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))                         # residual connection\n",
        "        x = x + self.ffwd(self.ln2(x))                       # residual connection\n",
        "        return x"
      ],
      "metadata": {
        "id": "kS7OcGYrg55f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the main language model class having numto\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)           # vocab_size --> n_embed\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)        # block_size --> n_embed (because positional embeddings)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # multiple blocks in sequence with each block having self-attention, feed-forwaed, layer-norm etc\n",
        "        self.ln_f = nn.LayerNorm(n_embd)                                        # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)                            # final linear layer\n",
        "        self.apply(self._init_weights)                                          # initialize the weights\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape                                                        # batch_size, block_size\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)                               # (batch_size, block_size, n_embed)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (block_size, n_embed)\n",
        "        x = tok_emb + pos_emb                                                   # (batch_size, block_size, n_embed) via brodcasting\n",
        "        x = self.blocks(x)                                                      # (batch_size, block_size, n_embed)\n",
        "        x = self.ln_f(x)                                                        # (batch_size, block_size, n_embed)\n",
        "        logits = self.lm_head(x)                                                # (batch_size, block_size, channels (vocab_size=65))\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]                                     # crop idx to the last block_size tokens\n",
        "            logits, loss = self(idx_cond)                                       # get the predictions\n",
        "            logits = logits[:, -1, :]                                           # focus only on the last time step --> becomes (B, C)\n",
        "            probs = F.softmax(logits, dim=-1)                                   # apply softmax to get probabilities --> still (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)                  # sample from the distribution --> becomes (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)                             # append sampled index to the running sequence --> (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "miDxd6kzhwTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the model\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "p5hGN8Vtin_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "faET8sX1j9v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDeYOP1ZiqmB",
        "outputId": "a5a31d9d-5c94-436b-cd26-9e9c9ef989a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start the timer\n",
        "training_start_time = time.time()"
      ],
      "metadata": {
        "id": "hImv4OajjMAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3StkcPUitNO",
        "outputId": "6548c810-e249-48df-e1e9-018dd3628e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2221, val loss 4.2306\n",
            "step 500: train loss 1.7600, val loss 1.9146\n",
            "step 1000: train loss 1.3903, val loss 1.5987\n",
            "step 1500: train loss 1.2644, val loss 1.5271\n",
            "step 2000: train loss 1.1835, val loss 1.4978\n",
            "step 2500: train loss 1.1233, val loss 1.4910\n",
            "step 2999: train loss 1.0733, val loss 1.4809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop the timer\n",
        "training_stop_time = time.time()"
      ],
      "metadata": {
        "id": "Ywii3_0yjOkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Time taken for training: {training_stop_time - training_start_time}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M2VEPdQjR2j",
        "outputId": "32c0ef7c-20bb-4d07-b732-b84fb94cc089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for training: 1955.1144347190857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)        # kickstart the generation by choosing tensor([[0]])"
      ],
      "metadata": {
        "id": "YYBfhdOgi1IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Change the `max_new_tokens` to generate larger or smaller paragraphs"
      ],
      "metadata": {
        "id": "82RyAFo03u-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))    # generate the text using context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMcXfNMM3rGy",
        "outputId": "fdb804e9-7a23-4bf4-b6b0-357f26e3f976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Himself, being drument, or is nother, and then.\n",
            "\n",
            "KING RICHARD II:\n",
            "Go, let us touch of Lycusgh, busin English doth:\n",
            "Then, faee sinciate, enough they with so:\n",
            "These fasts that take down the little love.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "The trumph since, fair suns; and hasted, way, burn her!\n",
            "\n",
            "KING RICHARD III:\n",
            "In sure, I have side. Now, granted musicians!\n",
            "Come, good cousin, Claudio.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Cry, madam: I'll marry hand my cousin.\n",
            "\n",
            "DUKE OF YORK:\n",
            "\n",
            "KING RICHARD III:\n",
            "\n",
            "CATESBY:\n",
            "My queen Clarence is; and I muster shall advise you\n",
            "To you love my body's highnesst.\n",
            "\n",
            "EDWARD:\n",
            "His paint I swoon? and their marks now, since--\n",
            "\n",
            "KING EDNARY ?\n",
            "RICHARDARD:\n",
            "How will they stare go to have witne here\n",
            "Six you, look free now it.\n",
            "\n",
            "ISABELLA:\n",
            "See, I would for this the house you might of these\n",
            "Of what of her marriage:\n",
            "O you talk now, sir, and underately\n",
            "Your lord I did put here I left in ironate-to-like\n",
            "A stay attance in perck doul! 't be no guire.\n",
            "\n",
            "ANTIJ IITA:\n",
            "Go-more day, sir; he will make a chidren work:\n",
            "I have told him again. Di grace of Stanley;\n",
            "rich ghas a parel betimes in 's faith,\n",
            "He's aim's an amirch bot here strange to the Towes:\n",
            "He sugles the must stay by good morrow,\n",
            "Hold me, instrume i tseal the roche:\n",
            "Mine own horse! I did see: be he that broad,\n",
            "Or battler: I had raching her, now deceit,\n",
            "Is a firm, a prick march straight, in paint,\n",
            "For to place, alte down in the eastless.\n",
            "\n",
            "ANGELO:\n",
            "It is become the father from so, where state\n",
            "My presence souls I will swear to thee,\n",
            "Or shall I be to make my grace to thuse and there;\n",
            "So the most talk of mine as steal their blood\n",
            "Is treamplosable-penitent till, leave not\n",
            "At whom as mayself give hostly is with you.\n",
            "\n",
            "HASTINGS:\n",
            "Is's Bohemia.\n",
            "\n",
            "GLOUCESTER:\n",
            "What? no; no more, non.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "HASTINGS:\n",
            "Go, stay, brother.\n",
            "\n",
            "KING EWIS OVERDONELIO:\n",
            "I am those happy to God, sir; and I\n",
            "let thee that e supper by trembling out on excuse pites,\n",
            "And that thou, for I can say then have fastion in thee\n",
            "Of so what give work against Sunried, do we get thee.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "That may my hol, tells shall be sure by allives.\n",
            "\n",
            "JOHN OF SURRENCE:\n",
            "But, grein right fellowse fiftin I be.\n",
            "\n",
            "YORK:\n",
            "The holy at Edward himself go; where's't myself?\n",
            "\n",
            "DUKE OF YORK:\n",
            "I brave my father, which in the Duke of York?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Give me ty then, in this grace\n",
            "Of our true that ake with me shock.\n",
            "\n",
            "DUKE OF YORK:\n",
            "That I did bear thy brother, and not hear,\n",
            "O, his fortune, thou know my chair;\n",
            "Where in pair back of the time that the better,\n",
            "Unlaws full and her prayers chame to be no strain,\n",
            "To seeing thy pursesies, the beam that wonder follows\n",
            "Here, my lord of Gloupelen and here and herself;\n",
            "To seven throughly shows at a leastness vitaes.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "I beseech you had exteempt your degresses are\n",
            "A friend tomicity thousand dog old;\n",
            "To thee craftion-men broke in that that sacrifice\n",
            "To make paralous ponion\n",
            "Is reside the enatops and his brow\n",
            "By the apre airs the place to make behll\n",
            "The which lap their tames murdere suppleasing.\n",
            "But Fifth place, to be Vtrotus, putting all acestom nost.\n",
            "Hand so nequair a wrayer breather retake the highs;\n",
            "Fetch a king fires, and crave I love;\n",
            "And now, unnaply groom more ination:\n",
            "I do beseech your request and fear to the state and the\n",
            "will of noble. What's the wounts Tybal\n",
            "With his Denro profpinciolical here,\n",
            "Her fathet spite of thing mother?\n",
            "She, Signior York, it shall wait low! Ispeak of.\n",
            "\n",
            "KING RICHARD III:\n",
            "Faith, my bag. What bame ishmendge for say?\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Heaven being tired how she come to France,\n",
            "Which on at the mor's pitck, lendow of day\n",
            "The wofundanger captives and shows bear\n",
            "Wherein Rivers, and fearful, neither\n",
            "Shall be sl ride with evillance: In smatter,\n",
            "In cribbodness, broth fight our praise doth shame,\n",
            "Let where an old mine father hand as to low,\n",
            "And that to intend things death. Methought I will pawnt me,\n",
            "To this agle hands and that beggars to end my deed,\n",
            "They are ugly till should angel will be speak,\n",
            "For her give to an Rosaline gold der, even trusties\n",
            "The pent.\n",
            "\n",
            "MENENIUS:\n",
            "And where's not\n",
            "My one deam honour, of blood; if he break you remembed\n",
            "That I have faintings, if she in Coriolanus\n",
            "And pardon'd of so the world, would have he's king.\n",
            "\n",
            "MENENIUS:\n",
            "Well, sir: but now my general brother,\n",
            "And so she woulding, who is you for strain,\n",
            "That Loddingrip unto his ol, and shall be stay.\n",
            "\n",
            "CORIOLANUS:\n",
            "My receipting!\n",
            "\n",
            "Thirs is i' the gime that pleasure he should thus?\n",
            "\n",
            "First Senator:\n",
            "No more apolect it, you cries in my threat firsthe;\n",
            "The poor glass musty wive touching. Give me have,\n",
            "The scraftaring thought is a judgment, trembling: that\n",
            "Beyondness and chaste Paulinant, must hath the present that\n",
            "Standies of railing: over-most more take\n",
            "Than ever balm answer, that is never back famility;\n",
            "And must all ham no imatter Than been.\n",
            "You seummer her; what I prawn the mercutless\n",
            "To too trasport of hold upon this farewell.\n",
            "\n",
            "HERMIONE:\n",
            "It is one thig long prevengeal: but, like Meneniever, let's capta,\n",
            "My if mocks of she might contemptent shoul as\n",
            "Are stoed against from leash; her\n",
            "And dum never confidence, never burthen's nature.\n",
            "Our gods, breather, not too bide.\n",
            "\n",
            "LEONTES:\n",
            "Let us not mend in this lamenly, business,\n",
            "Subdured me 's he.\n",
            "\n",
            "LEONTES:\n",
            "Come, my lord.\n",
            "\n",
            "MARIANGE:\n",
            "Ay, my good, a maid 'men.\n",
            "\n",
            "LADY LT:\n",
            "The very lord and god!\n",
            "\n",
            "LADY CAPULET:\n",
            "\n",
            "AgTES:\n",
            "This well at that while at a whorst stay.\n",
            "\n",
            "JULIET:\n",
            "I have need; and when I had been to touched.\n",
            "\n",
            "LADY CAPULET:\n",
            "Then, but to move his uncle to my father;\n",
            "Third if the court of sweet coverts, on I swood.\n",
            "\n",
            "LADY CAPULET:\n",
            "As she makes main, and I should have hot mistrad\n",
            "And as you kneely: let us take away?\n",
            "\n",
            "CAPULET:\n",
            "Twenty's unhead, will your babes marriage be profanite.\n",
            "\n",
            "LADY CAPULET:\n",
            "Come, fellow me, fetcher, that went all things\n",
            "Which do I will-give to this bawdly's.\n",
            "\n",
            "CAPULET:\n",
            "Jesu invher to the senate years, the last with nearle\n",
            "Of the duke changedly thel winte or ail,\n",
            "When I shall be atlement.\n",
            "\n",
            "LADY CAPULET:\n",
            "Neither, my glory wife, nover, from she spirits,\n",
            "To pardon him for the duke that shall see on thee.\n",
            "\n",
            "CAPULINT:\n",
            "Gentle CApules!\n",
            "\n",
            "CAPULET:\n",
            "That I must be talk'd both line heread,\n",
            "With that dear she depatrets on them nimbers\n",
            "To stop what's proudles fight and rebr\n",
            "With some rous will town: when is Hastings,\n",
            "For those stailors are he for'Where not a vessel'd.\n",
            "But if that she be dead, I'll draily and to my ble true\n",
            "Shall heaven have no more: away, madam less,\n",
            "And so last my thing sifies and do, hard it like.\n",
            "Herr cold the three-husband by to this life lengten news!\n",
            "Poor such and count the haple comes\n",
            "Which prosperil tempare of his wab, and within myself,\n",
            "Live with France to his state turn and him der preyment,\n",
            "Could not well, nor servitoriously to\n",
            "Plantagenet to the Coriolanus,\n",
            "No cot away: and, come buy prize which,\n",
            "We should, regimend him to a better fled\n",
            "Be haghting for twom hambite: and would since away;\n",
            "For either tribes: nay never with happy doorn.\n",
            "That remember thou marvell's knock of than.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "A my garefty lord casting to me; now, Edward!\n",
            "\n",
            "KING RICHARD III:\n",
            "Give un the part so mean with Tiba's wol towe\n",
            "That Edward's grace!\n",
            "\n",
            "CLARENCE:\n",
            "Pardon my innocent post sees me must be my lote:\n",
            "I'll had you so, thou must case i' the worth:\n",
            "But call me news, for that I strand m you all;\n",
            "And I so, dear God brother pardon with kill\n",
            "Thy love in gold. The stands beggard Tybalt in this in,\n",
            "When I have, the house cause was listed,\n",
            "Nonly, ay, as my relish cheeks on it.\n",
            "\n",
            "JULIET:\n",
            "The great name that sun's touchet, nor say 'Bless 'Beshreptell thee,\n",
            "To choose this not as my haste wexecution is nor\n",
            "By jest anholy marriage him,\n",
            "Be these debasent, and simpress it sorred;\n",
            "If he be too fanted to from the swift tears.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "A bless-tench; and, as I would not leckledge:\n",
            "Yet that at Pomphet, is being them anto tarm'd\n",
            "In doubt the children blow? The bribe'st through\n",
            "Draw how by fear of succh mmonths, thell\n",
            "That newly should make a small tot steel in't:\n",
            "As all thm carriagates and haise it;\n",
            "But shall in all it end.\n",
            "\n",
            "First Musiciane:\n",
            "Here are trantor some.\n",
            "\n",
            "First Senator:\n",
            "No; I, pray, in some fare?\n",
            "\n",
            "All:\n",
            "Welcome ne'er.\n",
            "\n",
            "Clown:\n",
            "He sir, farewell; I beseech yoke the manner\n",
            "I shall before you, and see thee hand.\n",
            "\n",
            "MERCUTIO:\n",
            "Madam.\n",
            "\n",
            "ARCIUS:\n",
            "Ay, masterss, he must: by my king, and and great affect.\n",
            "\n",
            "Servant:\n",
            "Reproof. Is am ilike ore your thumberland, where be it\n",
            "The provost repiercing begins: I shall sthick it,\n",
            "And call'st as this lambs: therefore, for the pish sins;\n",
            "But with eagh great Venusia creation,\n",
            "Finds, some leisure their brother; bold subder them\n",
            "wipe a goldener of to some rich wash each in need.\n",
            "\n",
            "HERMIONEA:\n",
            "Non you bring our queen nichther's bald nom\n",
            "Lucenting to your lady. Lady Catesby: ladies\n",
            "The letter wells your black in Bohem to residence.\n",
            "\n",
            "LEONTES:\n",
            "How thanks? When, Cleomberla haste noble and now.\n",
            "The sknealths calls 'Stears her, that faires hangs the\n",
            "wretcks, not unshear which charged.\n",
            "What says you hither? There's elder?\n",
            "\n",
            "PAULINA:\n",
            "So,\n",
            "When she hath our words? Sit, we think, poor\n",
            "ia, her.\n",
            "\n",
            "PAULINA:\n",
            "I am a sure, I return the carhy.\n",
            "\n",
            "DORCAS:\n",
            "Mark, there womfort is a felmer's in pitrion?\n",
            "\n",
            "CAMILLO:\n",
            "Pardon, fairer, and endures to see the hath last\n",
            "Our denient enemy's firm to him. Hear no work what knock!\n",
            "\n",
            "POMPEY:\n",
            "It is talk, sir, and sifter the compassion is.\n",
            "\n",
            "EABRAHISARISTA:\n",
            "Well, the streetst arms, 'tis an at appet.\n",
            "\n",
            "CLARENCE:\n",
            "An one that of air sold wolves by the way\n",
            "Since Paulina; wherein you will malicate\n",
            "With true that yourself, wherein you respect hanged\n",
            "To answer mew'd him young senfect.\n",
            "\n",
            "POMPEY:\n",
            "You shall promise affection with you,\n",
            "which ear s your knock, ere should not perchance since,\n",
            "your choices littly.\n",
            "\n",
            "ESCALUS:\n",
            "Of son's is great, no more, nor light.\n",
            "\n",
            "First Servingman:\n",
            "Yes, lords, and prant could not that fear\n",
            "asside call the king: is the poor tops\n",
            "for and your fanctiony are: do your since by boast.\n",
            "\n",
            "AUTOLYCUS:\n",
            "They would call hubour underss his action\n",
            "clearft.\n",
            "\n",
            "Servost:\n",
            "They hard ever but before they\n",
            "where, unstanding better I deny.\n",
            "\n",
            "LUCIO:\n",
            "Patter, but thank that, I'll have dribht my babe: be Margaris, discond\n",
            "mours merring with ervery though first, partly.\n",
            "\n",
            "DUKE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c3l83mVvDBzW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}